{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Linear regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmortuza/machine-learning/blob/master/models/regression/linear_regression/Linear%20regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6fJq5h7md5y"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmortuza/machine-learning/blob/master/2.%20Machine%20Learning/2.1%20Regression/Linear%20regression.ipynb\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEMbD2Romd5z"
      },
      "source": [
        "# Linear regression\n",
        "Linear regression is one of the most widely used machine learning model. In this notebook we will see how to use implement linear regression using the python. We will use boston housing price datasets to validate our algorithm. \n",
        "### Hypothesis\n",
        "To perform supervised learning, we must decide how we're going to represent functions/hypothesis $ h $ in a computer. As an initial choice lets say we decide to approximate $ y $ as a linear function of $ x $.\n",
        "$$ y = h_w(x) = w_0 + w_1x_1 + w_2x_2 + ..... $$\n",
        "Here $ w_i $ are the **parameters**(also called **weights**) parameterizing the space of linear functions mapping from $ \\mathcal{X} $ to $ \\mathcal{Y} $. Here, $ x_i $ represent the input feature and $ w_i $ represent how important that particular feature is for the output. For example size of the house might be the most important feature for the price so weight related this feature will be higher.  We can write the equations more compactly:\n",
        "$$ h(x) = \\sum_{i=0}^nw_ix_i = w^Tx $$\n",
        "There are multiple ways of updating the weights. For example gradient descent, using normal equations, newton's method etc.\n",
        "### Cost function\n",
        "We need to set $ w $ in a way that it makes the value of $ h(x) $ close to $ y $, at least for a single training example. We can do that by reducing the mean square loss between $ h(x) $ and $ y $. So we will keep updating the value of $ w $ untill $ (h(x) - y)^2 \\approx 0 $. This is a loss function for a single training example. For $ m $ training example the **Cost function** will be\n",
        "$$ J(w) = \\frac{1}{2}\\sum_{i=0}^m(h_w(x^{(i)}) - y^{(i)})^2 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXQ8mmGJmd50"
      },
      "source": [
        "## Implement the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXSTgsBimd50"
      },
      "source": [
        "# Import the required module\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoAC-zUQmd54"
      },
      "source": [
        "### Gradient descent\n",
        "$$ w_j := w_j - \\alpha \\frac{\\partial{J(w)}}{\\partial{w}} $$\n",
        "$$ \\frac{\\partial{J(w)}}{\\partial{w}} = \\sum_{i=1}^m(h_w(x^{(i)}) - y^{(i)})x_j^{(i)} $$\n",
        "So the update rule for the gradient descent will be\n",
        "$$\n",
        "\\text{Repeat until convergence}\\{ \\\\\n",
        "   \\;\\;\\;\\;\\;\\; w_j := w_j - \\alpha \\sum_{i=1}^m(h_w(x^{(i)}) - y^{(i)})x_j^{(i)} \\\\\n",
        "\\} \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py_Aw31bmd54"
      },
      "source": [
        "def propagate(w, b, x, y):\n",
        "    m = x.shape[0]\n",
        "    # Calculating cost\n",
        "    h_x = np.dot(x, w) + b\n",
        "    cost = np.sum((h_x - y) ** 2) / 2\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = np.dot(x.T, (h_x - y)) / m\n",
        "    db = np.sum(h_x - y) / m\n",
        "\n",
        "    return dw, db, cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjWs6S_amd57"
      },
      "source": [
        "def optimize(w, b, x, y, learning_rate, num_iteration, verbose):\n",
        "    costs = {}\n",
        "    for i in range(num_iteration):\n",
        "        dw, db, cost = propagate(w, b, x, y)\n",
        "        # Updating parameter\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            costs[i] = cost\n",
        "            if verbose:\n",
        "                print(\"Cost after iteration \", i, \" is \", cost)\n",
        "    return w, b, costs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI3YM0ixmd5-"
      },
      "source": [
        "### Normal equation\n",
        "By using the an equation we can find the value of $ w $ that will minimize the $ J(w) $. The equation can be derived by using the property of linear algebra.\n",
        "\n",
        "$$\n",
        "w = (X^TX)^{-1}X^T\\vec{v}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mybwQ9WLmd5-"
      },
      "source": [
        "def optimize_by_normal_equation(x, y):\n",
        "    return np.dot(np.dot(np.linalg.inv(np.dot(x.T, x)), x.T), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K1ZUfrjmd6B"
      },
      "source": [
        "### Combine all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-sW2NZvmd6C"
      },
      "source": [
        "def predict(w, b, x):\n",
        "    return np.dot(x, w) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n67591Ppmd6F"
      },
      "source": [
        "def model(x_train, y_train, x_test, y_test, method=\"gradient_descent\", learning_rate=0.001, num_iteration=5000, verbose=False):\n",
        "    # Resizing the y input\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "    # initialize parameters with zeros\n",
        "    w = np.zeros((x_train.shape[1], 1))\n",
        "    b = 0\n",
        "\n",
        "    w, b, costs = optimize(w, b, x_train, y_train, learning_rate, num_iteration, verbose)\n",
        "    if method == \"normal_equation\":\n",
        "        w = optimize_by_normal_equation(x_train, y_train)\n",
        "    y_prediction_train = predict(w, b, x_train)\n",
        "    y_prediction_test = predict(w, b, x_test)\n",
        "    train_r1 = r2_score(y_train, y_prediction_train)\n",
        "    test_r1 = r2_score(y_test, y_prediction_test)\n",
        "    print(\"Train f1 score: \", train_r1)\n",
        "    print(\"Test f1 score: \", test_r1)\n",
        "\n",
        "    return {\"costs\": costs,  \"train_f1\": train_r1, \"test_f1\": test_r1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRRlvQinmd6J"
      },
      "source": [
        "## Test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whLAZC-Xmd6J",
        "outputId": "e59d3cef-77af-4a7a-892d-b7cd409cd8c0"
      },
      "source": [
        "data = load_boston()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)\n",
        "X_train = preprocessing.scale(X_train)\n",
        "X_test = preprocessing.scale(X_test)\n",
        "history = model(X_train, y_train, X_test, y_test)\n",
        "plt.plot(list(history[\"costs\"].keys()), list(history[\"costs\"].values()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train f1 score:  0.7426457448245715\n",
            "Test f1 score:  0.6727000429784762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih5-MFL3md6N",
        "outputId": "73c783c0-d47b-4c14-d2d4-a0d3e22d3a0e"
      },
      "source": [
        "# Test with normal equation\n",
        "history = model(X_train, y_train, X_test, y_test, method=\"normal_equation\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train f1 score:  0.7511784126285308\n",
            "Test f1 score:  0.6762276430987266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGbtEj6bmd6Q"
      },
      "source": [
        "## Compare with sikit learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3oy4ZiLmd6Q",
        "outputId": "2717a23b-272d-44b6-8e25-bad1d8fbf817"
      },
      "source": [
        "linearRegression = LinearRegression()\n",
        "linearRegression.fit(X_train, y_train)\n",
        "print(\"Train r2 score: \", r2_score(y_train, linearRegression.predict(X_train)))\n",
        "print(\"Test r2 score: \", r2_score(y_test, linearRegression.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train r2 score:  0.75142102292153\n",
            "Test r2 score:  0.6760908223042545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEoy9IGDmd6T"
      },
      "source": [
        "## Locally weighted linear regression\n",
        "By linear regression it is not possible to predict the output of any dataset if the dataset doesn't maintain linear relationship between input and output. Locally weighted linear regression can be a great help in this case. it doesn't consider the entire dataset for prediction. Instead it only use the dataset which are close to the input which need to be predicted. For example, If we forcast the weather of 1st january, 2020 and if we use the linear regression it will take the every month's weather into consideration. But it would be best if we only consider the weather of nearby date of 1st january. \n",
        "![Locally weighted linear regression](https://github.com/gmortuza/machine-learning/blob/master/models/regression/linear_regression/img/locally_weighted.png?raw=1)\n",
        "It is impossible to fit linear line into this dataset but it is possible to fit a non-linear line into this dataset. It is a **non-parametric** learning algorithm . (Algorithms that do not make strong assumptions about the form of the mapping function are called nonparametric machine learning algorithms. By not making assumptions, they are free to learn any functional from the training data.) Linear regression is **parametric** learning algorithm where we learn the mapping function/hypothesis and throws away the training data.\n",
        "\n",
        "In linear regression we fit $ w $ to minimzie:\n",
        "$$ \\sum_{i=1}^m(y^{(i)} - w^Tx^{(i)})^{(i)} $$\n",
        "In locally weighted linear regression we will fit $ w $ to minimize:\n",
        "$$ \\sum_{i=1}^m\\mathcal{l}^{(i)}(y^{(i)} - w^Tx^{(i)})^{(i)} $$\n",
        "$$ \\mathcal{l}^{(i)} = exp(- \\frac{(x^{(i)} - x)^2}{2\\tau^2}) $$\n",
        "Here $ x $ is the input for which we want to predict output. $ \\mathcal{l}^{(i)} \\approx 1 $ if $ |x^{(i)} - x| $ is small. And for training example nearby $ x $, $ |x^{(i)} - x| $ will be small. So those point will be consider for prediciton only. $ \\tau $ is called the **bandwidth** parameter. It's hypyerparameter that will tell the algorithm about how many training example it should consider for prediction."
      ]
    }
  ]
}