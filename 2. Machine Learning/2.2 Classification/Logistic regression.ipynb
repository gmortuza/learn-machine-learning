{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "By the name it might seem like Logistic regression is a regression(output value is continious) problem. But the real case is logistic regression is a classification problem(output value is discrete). More specifically logistic regression is a binary classification problem. In this notebook we will deep dive into the mathemetics behind the logistic regression. Also we will see the implementation of logistic regression. Finally, we will measure our performance on breast cancer dataset. And we will compare our performance with scikit-learn logistic regression implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis \n",
    "Before implementing the logistic regression we must decide how we are going to represent our function/hypothesis $ h $ in a computer. Let's assume we decided to approximate y as a linear function of x:\n",
    "$$ y(x) = h_w(x) = b + w_0x_0 + w_0x_0 + ....... $$\n",
    "Here, $ w_i $'s are the *parameter* this is also known as *weights*. And $ x_i $ represents the input features. If we have a n number of input features then we can simplify the notation as:\n",
    "\n",
    "$$ h(x) = \\sum_{i=0}^n w_ix_i = w^Tx + b $$\n",
    "\n",
    "This hypothesis will return output between [$ -\\infty, +\\infty $]. But the output of the logistic regression is binary, it's either 0 or 1 $ y \\in \\{0, 1\\} $. So we need a function that will squizee any value between 0 and 1. *Sigmoid function* is one of the function that does this thing for us. The sigmoid function is \n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$ \n",
    "$$ \\sigma(h_w(x)) = \\sigma(w^Tx + b) = \\frac{1}{1 + e^{-(w^Tx + b)}} $$\n",
    "Let's just implement this function and plot it's graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sigmoid function')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    z = 1/(1 + np.exp(-z))\n",
    "    return z\n",
    "z = np.linspace(-10, 10, 100)   \n",
    "plt.plot(z, sigmoid(z)) \n",
    "plt.xlabel(\"z\") \n",
    "plt.ylabel(\"g(z)\") \n",
    "plt.title(\"Sigmoid function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can clearly see that regardless of the input value, output value is between 0 and 1. Instead of the sigmoid function any other function can be used. But sigmoid function is most widely used because of the certain benefits. You can learn more about the benefits of sigmoid function [here](https://sebastianraschka.com/faq/docs/logistic-why-sigmoid.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "Cost function is used to measure the performance of any machine learning model. It quantifies the error between the predicted value and expected value. So, our goal is to reduce the error of the overall model. There are set of value of $ w $ that will minimize the value of cost function. We need to find that $ w $ value.\n",
    "Let's assume that:\n",
    "Probability of $ y = 1 $ given x is $ h_w(x) $; where the parameters are $ w $\n",
    "$$ P(y = 1 | x; w) = h_w(x) $$\n",
    "As it's a binary classification so the probability of $ y = 0 $ given x will be $ 1 - h_w(x) $\n",
    "$$ P(y = 0 | x; w) = 1 - h_w(x) $$\n",
    "This can be rewritten compactly as:\n",
    "$$ P(y | x; w) = (h_w(x))^y(1-h_w(x))^{(1-y)} $$\n",
    "\n",
    "Assuming that we have $ m $ training examples which are independent with each other, we can write down the likelihood of the parameter as\n",
    "$$ \n",
    "\\begin{align}\n",
    "L(w) &= p(y | X; w) \\\\\n",
    "& = \\prod_{i=1}^m p(y^{(i)} | x^{(i)}; w) \\\\\n",
    "& = \\prod_{i=1}^m (h_w(x^{(i)}))^{y^{(i)}}(1-h_w(x^{(i)}))^{(1-y)^{(i)}}\n",
    "\\end{align} \n",
    "$$ \n",
    "\n",
    "This equation contains the product of many probability. And the probability is a small number which is less than number. Multiplying many small number might make the resulting number close to 0. So it would be hard to maximize/minimize that. So it's better to convert the prodcut into summation. For that instead of maximizing/minimizing the likelihood we will maximize/minimize the log likelihood\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell(w) &= log L(w) \\\\\n",
    "&= \\sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1 - y^{(i)}) log(1-h(x^{(i)}))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Instead of maximizing the log likelihood we can also minimize the negetive of log likelihood. Both works same. But the name cost is something like it should be reduced so we are minimizing it.\n",
    "\n",
    "$$ \\ell(w) = - \\sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1 - y^{(i)}) log(1-h(x^{(i)})) $$\n",
    "```python\n",
    "a = h_w(x) = sigmoid(np.dot(x, w) + b)\n",
    "cost = - np.sum(y *(np.log(a) + (1-y) * np.log(1-a)) / m\n",
    "```\n",
    "we can minimize this using the [gradient descent](https://www.youtube.com/watch?v=IHZwWFHWa-w). For that we need to take the derivate of the cost function. \n",
    "$$ \\frac{\\partial \\ell(w)}{\\partial w_j} = (h_w(x) - y)x_j $$\n",
    "```python\n",
    "\n",
    "dw = np.dot(x.T, (a-y)) / m\n",
    "db = np.sum(a -y) /m\n",
    "```\n",
    "The update rule for our parameter w is: \n",
    "$$ w_j := w_j - \\alpha \\frac{\\partial \\ell(w)}{\\partial w_j} $$\n",
    "\n",
    "```python\n",
    "w = w - alpha * dw\n",
    "b = b - alpha * db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Architecture of the learning algorithm ##\n",
    "![lg cat](./lg_cat.png)\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, x, y):\n",
    "        \"\"\"\n",
    "        Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "        :param: w -- weights, a numpy array of size (x.shape[1], 1). Number of feature of x\n",
    "        :param: b -- bias, a scalar\n",
    "        :param: x -- data of size (number_of_training_example, number of feature)\n",
    "        :param: y -- true \"label\" vector of size (number_of_training_example, 1)\n",
    "\n",
    "        :returns: cost -- negative log-likelihood cost for logistic regression\n",
    "        :returns: dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "        :returns: db -- gradient of the loss with respect to b, thus same shape as b\n",
    "        \"\"\"\n",
    "\n",
    "        m = x.shape[0]  # Number of training example\n",
    "\n",
    "        # FORWARD PROPAGATION (FROM x TO COST)\n",
    "        a = sigmoid(np.dot(x, w) + b)  # compute activation\n",
    "        cost = - np.sum(y * np.log(a) + (1 - y) * np.log(1 - a)) / m  # compute cost\n",
    "        # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "        dw = np.dot(x.T, (a - y)) / m\n",
    "        db = np.sum(a - y) / m\n",
    "\n",
    "        cost = np.squeeze(cost)\n",
    "\n",
    "        grads = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "        return grads, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, x, y, num_iteration, learning_rate, verbose):\n",
    "        \"\"\"\n",
    "        This function optimizes w and b by running a gradient descent algorithm\n",
    "\n",
    "        Arguments:\n",
    "        w -- weights, a numpy array of size (x.shape[1], 1). Number of feature of x\n",
    "        b -- bias, a scalar\n",
    "        x -- data of shape (number_of_training_example, number of feature)\n",
    "        y -- true \"label\" vector, of shape (1, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        params -- dictionary containing the weights w and bias b\n",
    "        grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "        costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "        \"\"\"\n",
    "\n",
    "        costs = {}\n",
    "\n",
    "        for i in range(num_iteration):\n",
    "\n",
    "            # Cost and gradient calculation\n",
    "            grads, cost = propagate(w, b, x, y)\n",
    "\n",
    "            # Retrieve derivatives from grads\n",
    "            dw = grads[\"dw\"]\n",
    "            db = grads[\"db\"]\n",
    "\n",
    "            # update rule\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "            # Record the costs\n",
    "            if i % 100 == 0:\n",
    "                costs[i] = cost\n",
    "\n",
    "            # Print the cost every 100 training iterations\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "        return w, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "        \"\"\"\n",
    "        Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "        Arguments:\n",
    "        w -- weights, a numpy array of size (x.shape[1], 1). Number of feature of x\n",
    "        b -- bias, a scalar\n",
    "        x -- data of size (number_of_training_example, number of feature)\n",
    "\n",
    "        Returns:\n",
    "        y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in x\n",
    "        \"\"\"\n",
    "\n",
    "        m = x.shape[0]\n",
    "        y_prediction = np.zeros((m, 1))\n",
    "        w = w.reshape(x.shape[1], 1)\n",
    "\n",
    "        # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "        a = sigmoid(np.dot(x, w) + b)\n",
    "\n",
    "        for i in range(a.shape[0]):\n",
    "            # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "            y_prediction[i][0] = 0 if a[i][0] <= .5 else 1\n",
    "\n",
    "        return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def model(x_train, y_train, x_test, y_test, num_iterations = 2000, learning_rate = 0.005, verbose = True):\n",
    "        \"\"\"\n",
    "        Builds the logistic regression model by calling the function you've implemented previously\n",
    "\n",
    "        Arguments:\n",
    "        x_train -- training set represented by a numpy array of shape (number_of_training_example, number of feature)\n",
    "        y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "        x_test -- test set represented by a numpy array of shape (number_of_training_example, number of feature)\n",
    "        y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "        num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "        learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "        print_cost -- Set to true to print the cost every 100 iterations\n",
    "\n",
    "        Returns:\n",
    "        d -- dictionary containing information about the model.\n",
    "        \"\"\"\n",
    "        # Resizing the y input\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_test = y_test.reshape(-1, 1)\n",
    "        # initialize parameters with zeros\n",
    "        w = np.zeros((x_train.shape[1],1))\n",
    "        b = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        w, b, costs = optimize(w, b, x_train, y_train, num_iterations, learning_rate, verbose)\n",
    "        y_prediction_test = predict(x_test, w, b)\n",
    "        y_prediction_train = predict(x_train, w, b)\n",
    "        train_acc = 100 - np.mean(np.abs(y_prediction_train - y_train)) * 100\n",
    "        test_acc = 100 - np.mean(np.abs(y_prediction_test - y_test)) * 100\n",
    "        train_f1 = f1_score(y_train, y_prediction_train)\n",
    "        test_f1 = f1_score(y_test, y_prediction_test)\n",
    "        print(\"Train f1 score: \", train_f1)\n",
    "        print(\"Test f1 score: \", test_f1)\n",
    "\n",
    "        return {\"costs\": costs, \"train_accuracy\": train_acc, \"test_accuracy\": test_acc, \"train_f1\": train_f1,\n",
    "                \"test_f1\": test_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df[\"target\"] = data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = preprocessing.scale(X_train)\n",
    "X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (381, 30)\n",
      "y_train:  (381,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"y_train: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score:  0.9853249475890986\n",
      "Test f1 score:  0.9917355371900827\n"
     ]
    }
   ],
   "source": [
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 2000, learning_rate = 0.005, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feb4ac5e4d0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(d[\"costs\"].keys()), list(d[\"costs\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (learn-machine-learning)",
   "language": "python",
   "name": "pycharm-9ceae2e3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
